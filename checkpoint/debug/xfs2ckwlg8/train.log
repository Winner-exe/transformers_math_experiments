INFO - 01/04/25 21:41:07 - 0:00:00 - ============ Initialized logger ============
INFO - 01/04/25 21:41:07 - 0:00:00 - batch_size: 32
                                     command: python C:\Users\Winston\Documents\Math-AI\transformers_math_experiments\fc_loop.py --num_initial_empty_objects=40000 --final_database_size=10000 --target_db_size=40000 --sample-only=40000 --nb_threads=16 --nb_local_searches=3200 --num-workers=20 --max-steps=20000 --max_epochs=30000 --seed=-1 --top-k=-1 --type=transformer --n-layer=4 --n-head=8 --n-embd=64 --n-embd2=32 --batch-size=32 --learning-rate=5e-4 --weight-decay=0.01 --max-output-length=160 --gen_batch_size=1000 --n_tokens=100 --temperature=1.0 --dump_path=checkpoint --exp_name=debug --local_rank=-1 --master_port=-1 --cpu=false --debug_slurm=False --exp_id "xfs2ckwlg8"
                                     cpu: False
                                     debug: False
                                     debug_slurm: False
                                     dump_path: checkpoint\debug\xfs2ckwlg8
                                     exp_id: xfs2ckwlg8
                                     exp_name: debug
                                     final_database_size: 10000
                                     gen_batch_size: 1000
                                     global_rank: 0
                                     is_master: True
                                     is_slurm_job: False
                                     learning_rate: 0.0005
                                     local_rank: 0
                                     master_port: -1
                                     max_epochs: 30000
                                     max_output_length: 160
                                     max_steps: 20000
                                     multi_gpu: False
                                     multi_node: False
                                     n_embd: 64
                                     n_embd2: 32
                                     n_gpu_per_node: 1
                                     n_head: 8
                                     n_layer: 4
                                     n_nodes: 1
                                     n_tokens: 100
                                     nb_local_searches: 3200
                                     nb_threads: 16
                                     node_id: 0
                                     num_initial_empty_objects: 40000
                                     num_workers: 20
                                     sample_only: 40000
                                     seed: -1
                                     target_db_size: 40000
                                     temperature: 1.0
                                     top_k: -1
                                     type: transformer
                                     weight_decay: 0.01
                                     world_size: 1
INFO - 01/04/25 21:41:07 - 0:00:00 - The experiment will be stored in checkpoint\debug\xfs2ckwlg8
                                     
INFO - 01/04/25 21:41:07 - 0:00:00 - Running command: python C:\Users\Winston\Documents\Math-AI\transformers_math_experiments\fc_loop.py --num_initial_empty_objects=40000 --final_database_size=10000 --target_db_size=40000 --sample-only=40000 --nb_threads=16 --nb_local_searches=3200 --num-workers=20 --max-steps=20000 --max_epochs=30000 --seed=-1 --top-k=-1 --type=transformer --n-layer=4 --n-head=8 --n-embd=64 --n-embd2=32 --batch-size=32 --learning-rate=5e-4 --weight-decay=0.01 --max-output-length=160 --gen_batch_size=1000 --n_tokens=100 --temperature=1.0 --dump_path=checkpoint --exp_name=debug --local_rank=-1 --master_port=-1 --cpu=false --debug_slurm=False

INFO - 01/04/25 21:41:07 - 0:00:00 - seed: 932421605
INFO - 01/04/25 21:41:07 - 0:00:00 - JULIA_NUM_THREADS is set to 16
INFO - 01/04/25 21:41:19 - 0:00:12 - Created checkpoint\debug\xfs2ckwlg8\temp.txt and training tokenizer...
INFO - 01/04/25 21:41:19 - 0:00:12 - Directory 'checkpoint\debug\xfs2ckwlg8/tokenizer_data' created.
INFO - 01/04/25 21:41:20 - 0:00:13 - File 'checkpoint\debug\xfs2ckwlg8\temp.txt' has been deleted.
INFO - 01/04/25 21:41:20 - 0:00:13 - 0 / 10000
INFO - 01/04/25 21:41:20 - 0:00:14 - initializing at generation: 1
INFO - 01/04/25 21:41:20 - 0:00:14 - number of examples in the dataset: 10000
INFO - 01/04/25 21:41:20 - 0:00:14 - max word length: 42
INFO - 01/04/25 21:41:20 - 0:00:14 - number of unique characters in the vocabulary: 100
INFO - 01/04/25 21:41:20 - 0:00:14 - vocabulary:
INFO - 01/04/25 21:41:20 - 0:00:14 - ['V0', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99']
INFO - 01/04/25 21:41:20 - 0:00:14 - split up the dataset into 9000 training examples and 1000 test examples
INFO - 01/04/25 21:41:20 - 0:00:14 - dataset determined that: vocab_size=101, block_size=161
INFO - 01/04/25 21:41:20 - 0:00:14 - model #params: 223296
INFO - 01/04/25 21:41:20 - 0:00:14 - ============ Start of generation 1 ============
INFO - 01/04/25 21:41:20 - 0:00:14 - Memory allocated:  1.26MB, reserved: 2.00MB
INFO - 01/04/25 21:41:20 - 0:00:14 - training
INFO - 01/04/25 21:41:57 - 0:00:50 - step 0 | loss 4.7936 | step time 1136.25ms
INFO - 01/04/25 21:42:00 - 0:00:54 - step 100 | loss 4.0365 | step time 39.12ms
INFO - 01/04/25 21:42:04 - 0:00:57 - step 200 | loss 3.9535 | step time 33.05ms
INFO - 01/04/25 21:42:08 - 0:01:01 - step 300 | loss 3.8348 | step time 44.80ms
INFO - 01/04/25 21:42:13 - 0:01:06 - step 400 | loss 3.8517 | step time 55.71ms
INFO - 01/04/25 21:42:18 - 0:01:11 - step 500 | loss 3.8633 | step time 39.98ms
INFO - 01/04/25 21:42:19 - 0:01:12 - step 500 train loss: 3.8437469005584717 test loss: 3.8561408519744873
INFO - 01/04/25 21:42:19 - 0:01:12 - test loss 3.8561408519744873 is the best so far, saving model to checkpoint\debug\xfs2ckwlg8\model.pt
INFO - 01/04/25 21:42:23 - 0:01:17 - step 600 | loss 3.8059 | step time 41.21ms
INFO - 01/04/25 21:42:29 - 0:01:22 - step 700 | loss 3.8040 | step time 56.25ms
INFO - 01/04/25 21:42:34 - 0:01:27 - step 800 | loss 3.8054 | step time 43.60ms
INFO - 01/04/25 21:42:38 - 0:01:31 - step 900 | loss 3.7123 | step time 33.49ms
INFO - 01/04/25 21:42:42 - 0:01:35 - step 1000 | loss 3.7703 | step time 32.35ms
INFO - 01/04/25 21:42:42 - 0:01:36 - step 1000 train loss: 3.7724075317382812 test loss: 3.7964797019958496
INFO - 01/04/25 21:42:42 - 0:01:36 - test loss 3.7964797019958496 is the best so far, saving model to checkpoint\debug\xfs2ckwlg8\model.pt
INFO - 01/04/25 21:42:46 - 0:01:39 - step 1100 | loss 3.7930 | step time 34.74ms
INFO - 01/04/25 21:42:50 - 0:01:43 - step 1200 | loss 3.8318 | step time 34.08ms
INFO - 01/04/25 21:42:53 - 0:01:46 - step 1300 | loss 3.7615 | step time 34.58ms
INFO - 01/04/25 21:42:56 - 0:01:50 - step 1400 | loss 3.7289 | step time 31.75ms
INFO - 01/04/25 21:43:00 - 0:01:53 - step 1500 | loss 3.6882 | step time 39.58ms
INFO - 01/04/25 21:43:01 - 0:01:54 - step 1500 train loss: 3.7273926734924316 test loss: 3.7727553844451904
INFO - 01/04/25 21:43:01 - 0:01:54 - test loss 3.7727553844451904 is the best so far, saving model to checkpoint\debug\xfs2ckwlg8\model.pt
INFO - 01/04/25 21:43:04 - 0:01:58 - step 1600 | loss 3.7408 | step time 36.96ms
INFO - 01/04/25 21:43:09 - 0:02:02 - step 1700 | loss 3.6800 | step time 32.04ms
INFO - 01/04/25 21:43:12 - 0:02:05 - step 1800 | loss 3.7218 | step time 32.26ms
INFO - 01/04/25 21:43:15 - 0:02:09 - step 1900 | loss 3.7236 | step time 33.78ms
INFO - 01/04/25 21:43:19 - 0:02:12 - step 2000 | loss 3.6448 | step time 31.73ms
INFO - 01/04/25 21:43:19 - 0:02:13 - step 2000 train loss: 3.7072389125823975 test loss: 3.753322124481201
INFO - 01/04/25 21:43:19 - 0:02:13 - test loss 3.753322124481201 is the best so far, saving model to checkpoint\debug\xfs2ckwlg8\model.pt
INFO - 01/04/25 21:43:23 - 0:02:16 - step 2100 | loss 3.6935 | step time 34.01ms
INFO - 01/04/25 21:43:26 - 0:02:19 - step 2200 | loss 3.6890 | step time 32.25ms
INFO - 01/04/25 21:43:29 - 0:02:22 - step 2300 | loss 3.7047 | step time 33.58ms
INFO - 01/04/25 21:43:32 - 0:02:26 - step 2400 | loss 3.6256 | step time 33.59ms
INFO - 01/04/25 21:43:36 - 0:02:29 - step 2500 | loss 3.6568 | step time 32.08ms
INFO - 01/04/25 21:43:36 - 0:02:30 - step 2500 train loss: 3.680039167404175 test loss: 3.742126941680908
INFO - 01/04/25 21:43:36 - 0:02:30 - test loss 3.742126941680908 is the best so far, saving model to checkpoint\debug\xfs2ckwlg8\model.pt
INFO - 01/04/25 21:43:40 - 0:02:33 - step 2600 | loss 3.7337 | step time 31.97ms
INFO - 01/04/25 21:43:43 - 0:02:36 - step 2700 | loss 3.6932 | step time 33.08ms
INFO - 01/04/25 21:43:47 - 0:02:40 - step 2800 | loss 3.6961 | step time 36.92ms
INFO - 01/04/25 21:43:50 - 0:02:44 - step 2900 | loss 3.6906 | step time 31.78ms
INFO - 01/04/25 21:43:54 - 0:02:47 - step 3000 | loss 3.6203 | step time 35.83ms
INFO - 01/04/25 21:43:54 - 0:02:48 - step 3000 train loss: 3.6560113430023193 test loss: 3.7342464923858643
INFO - 01/04/25 21:43:54 - 0:02:48 - test loss 3.7342464923858643 is the best so far, saving model to checkpoint\debug\xfs2ckwlg8\model.pt
INFO - 01/04/25 21:43:58 - 0:02:51 - step 3100 | loss 3.6421 | step time 36.18ms
INFO - 01/04/25 21:44:01 - 0:02:55 - step 3200 | loss 3.7716 | step time 35.15ms
