INFO - 01/14/25 22:32:52 - 0:00:00 - ============ Initialized logger ============
INFO - 01/14/25 22:32:52 - 0:00:00 - batch_size: 32
                                     command: python C:\Users\Winston\Documents\Math-AI\transformers_math_experiments\fc_loop.py --num_initial_empty_objects=40000 --final_database_size=10000 --target_db_size=40000 --sample-only=40000 --nb_threads=16 --nb_local_searches=3200 --num-workers=8 --max-steps=20000 --max_epochs=30000 --seed=-1 --top-k=-1 --type=transformer --n-layer=4 --n-head=8 --n-embd=64 --n-embd2=32 --batch-size=32 --learning-rate=5e-4 --weight-decay=0.01 --max-output-length=160 --gen_batch_size=2000 --n_tokens=100 --temperature=1.0 --dump_path=checkpoint --exp_name=debug --local_rank=-1 --master_port=-1 --debug_slurm=False --exp_id "r6ivwwgibx"
                                     cpu: False
                                     debug: False
                                     debug_slurm: False
                                     dump_path: checkpoint\debug\r6ivwwgibx
                                     exp_id: r6ivwwgibx
                                     exp_name: debug
                                     final_database_size: 10000
                                     gen_batch_size: 2000
                                     global_rank: 0
                                     is_master: True
                                     is_slurm_job: False
                                     learning_rate: 0.0005
                                     local_rank: 0
                                     master_port: -1
                                     max_epochs: 30000
                                     max_output_length: 160
                                     max_steps: 20000
                                     multi_gpu: False
                                     multi_node: False
                                     n_embd: 64
                                     n_embd2: 32
                                     n_gpu_per_node: 1
                                     n_head: 8
                                     n_layer: 4
                                     n_nodes: 1
                                     n_tokens: 100
                                     nb_local_searches: 3200
                                     nb_threads: 16
                                     node_id: 0
                                     num_initial_empty_objects: 40000
                                     num_workers: 8
                                     sample_only: 40000
                                     seed: -1
                                     target_db_size: 40000
                                     temperature: 1.0
                                     top_k: -1
                                     type: transformer
                                     weight_decay: 0.01
                                     world_size: 1
INFO - 01/14/25 22:32:52 - 0:00:00 - The experiment will be stored in checkpoint\debug\r6ivwwgibx
                                     
INFO - 01/14/25 22:32:52 - 0:00:00 - Running command: python C:\Users\Winston\Documents\Math-AI\transformers_math_experiments\fc_loop.py --num_initial_empty_objects=40000 --final_database_size=10000 --target_db_size=40000 --sample-only=40000 --nb_threads=16 --nb_local_searches=3200 --num-workers=8 --max-steps=20000 --max_epochs=30000 --seed=-1 --top-k=-1 --type=transformer --n-layer=4 --n-head=8 --n-embd=64 --n-embd2=32 --batch-size=32 --learning-rate=5e-4 --weight-decay=0.01 --max-output-length=160 --gen_batch_size=2000 --n_tokens=100 --temperature=1.0 --dump_path=checkpoint --exp_name=debug --local_rank=-1 --master_port=-1 --debug_slurm=False

INFO - 01/14/25 22:32:52 - 0:00:00 - seed: 19186624
INFO - 01/14/25 22:32:52 - 0:00:00 - JULIA_NUM_THREADS is set to 16
INFO - 01/14/25 22:33:06 - 0:00:14 - Created checkpoint\debug\r6ivwwgibx\temp.txt and training tokenizer...
INFO - 01/14/25 22:33:06 - 0:00:14 - Directory 'checkpoint\debug\r6ivwwgibx/tokenizer_data' created.
INFO - 01/14/25 22:33:07 - 0:00:16 - File 'checkpoint\debug\r6ivwwgibx\temp.txt' has been deleted.
INFO - 01/14/25 22:33:07 - 0:00:16 - 0 / 10000
INFO - 01/14/25 22:33:08 - 0:00:16 - initializing at generation: 1
INFO - 01/14/25 22:33:08 - 0:00:16 - number of examples in the dataset: 10000
INFO - 01/14/25 22:33:08 - 0:00:16 - max word length: 39
INFO - 01/14/25 22:33:08 - 0:00:16 - number of unique characters in the vocabulary: 100
INFO - 01/14/25 22:33:08 - 0:00:16 - vocabulary:
INFO - 01/14/25 22:33:08 - 0:00:16 - ['V0', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99']
INFO - 01/14/25 22:33:08 - 0:00:16 - split up the dataset into 9000 training examples and 1000 test examples
INFO - 01/14/25 22:33:08 - 0:00:16 - dataset determined that: vocab_size=101, block_size=161
INFO - 01/14/25 22:33:08 - 0:00:17 - model #params: 223296
INFO - 01/14/25 22:33:08 - 0:00:17 - ============ Start of generation 1 ============
INFO - 01/14/25 22:33:08 - 0:00:17 - Memory allocated:  1.26MB, reserved: 2.00MB
INFO - 01/14/25 22:33:08 - 0:00:17 - training
INFO - 01/14/25 22:33:30 - 0:00:39 - step 0 | loss 4.7830 | step time 843.99ms
INFO - 01/14/25 22:33:35 - 0:00:43 - step 100 | loss 4.0499 | step time 48.21ms
INFO - 01/14/25 22:33:39 - 0:00:48 - step 200 | loss 3.9779 | step time 43.75ms
INFO - 01/14/25 22:33:44 - 0:00:52 - step 300 | loss 3.8679 | step time 41.82ms
INFO - 01/14/25 22:33:48 - 0:00:56 - step 400 | loss 3.8375 | step time 42.46ms
INFO - 01/14/25 22:33:53 - 0:01:01 - step 500 | loss 3.8263 | step time 46.07ms
INFO - 01/14/25 22:33:54 - 0:01:02 - step 500 train loss: 3.8462984561920166 test loss: 3.857102870941162
INFO - 01/14/25 22:33:54 - 0:01:02 - test loss 3.857102870941162 is the best so far, saving model to checkpoint\debug\r6ivwwgibx\model.pt
INFO - 01/14/25 22:33:58 - 0:01:06 - step 600 | loss 3.8212 | step time 42.48ms
INFO - 01/14/25 22:34:02 - 0:01:11 - step 700 | loss 3.7737 | step time 38.99ms
INFO - 01/14/25 22:34:07 - 0:01:15 - step 800 | loss 3.7560 | step time 41.39ms
INFO - 01/14/25 22:34:11 - 0:01:19 - step 900 | loss 3.7409 | step time 40.99ms
INFO - 01/14/25 22:34:16 - 0:01:24 - step 1000 | loss 3.7440 | step time 47.21ms
INFO - 01/14/25 22:34:17 - 0:01:25 - step 1000 train loss: 3.7764694690704346 test loss: 3.792076587677002
INFO - 01/14/25 22:34:17 - 0:01:25 - test loss 3.792076587677002 is the best so far, saving model to checkpoint\debug\r6ivwwgibx\model.pt
INFO - 01/14/25 22:34:21 - 0:01:29 - step 1100 | loss 3.7885 | step time 43.76ms
INFO - 01/14/25 22:34:26 - 0:01:34 - step 1200 | loss 3.7573 | step time 64.80ms
INFO - 01/14/25 22:34:30 - 0:01:39 - step 1300 | loss 3.7069 | step time 57.17ms
INFO - 01/14/25 22:34:36 - 0:01:44 - step 1400 | loss 3.7146 | step time 44.65ms
INFO - 01/14/25 22:34:43 - 0:01:51 - step 1500 | loss 3.7215 | step time 75.86ms
INFO - 01/14/25 22:34:44 - 0:01:52 - step 1500 train loss: 3.7415578365325928 test loss: 3.7654194831848145
INFO - 01/14/25 22:34:44 - 0:01:52 - test loss 3.7654194831848145 is the best so far, saving model to checkpoint\debug\r6ivwwgibx\model.pt
INFO - 01/14/25 22:34:49 - 0:01:57 - step 1600 | loss 3.7619 | step time 65.48ms
INFO - 01/14/25 22:34:55 - 0:02:03 - step 1700 | loss 3.7188 | step time 102.39ms
INFO - 01/14/25 22:35:01 - 0:02:09 - step 1800 | loss 3.6826 | step time 40.29ms
INFO - 01/14/25 22:35:05 - 0:02:13 - step 1900 | loss 3.7071 | step time 42.42ms
INFO - 01/14/25 22:35:10 - 0:02:19 - step 2000 | loss 3.6984 | step time 168.62ms
INFO - 01/14/25 22:35:12 - 0:02:20 - step 2000 train loss: 3.7021634578704834 test loss: 3.751704454421997
INFO - 01/14/25 22:35:12 - 0:02:20 - test loss 3.751704454421997 is the best so far, saving model to checkpoint\debug\r6ivwwgibx\model.pt
INFO - 01/14/25 22:35:17 - 0:02:25 - step 2100 | loss 3.6523 | step time 43.53ms
INFO - 01/14/25 22:35:21 - 0:02:29 - step 2200 | loss 3.7023 | step time 48.73ms
INFO - 01/14/25 22:35:27 - 0:02:35 - step 2300 | loss 3.7551 | step time 47.45ms
INFO - 01/14/25 22:35:32 - 0:02:40 - step 2400 | loss 3.6811 | step time 46.64ms
INFO - 01/14/25 22:35:37 - 0:02:46 - step 2500 | loss 3.6750 | step time 56.13ms
INFO - 01/14/25 22:35:38 - 0:02:46 - step 2500 train loss: 3.6914689540863037 test loss: 3.75337290763855
INFO - 01/14/25 22:35:45 - 0:02:53 - step 2600 | loss 3.6730 | step time 47.69ms
INFO - 01/14/25 22:35:51 - 0:02:59 - step 2700 | loss 3.7049 | step time 48.36ms
INFO - 01/14/25 22:35:55 - 0:03:04 - step 2800 | loss 3.6825 | step time 51.72ms
INFO - 01/14/25 22:36:00 - 0:03:08 - step 2900 | loss 3.6832 | step time 45.96ms
INFO - 01/14/25 22:36:06 - 0:03:14 - step 3000 | loss 3.6916 | step time 81.37ms
INFO - 01/14/25 22:36:07 - 0:03:15 - step 3000 train loss: 3.6587753295898438 test loss: 3.7358691692352295
INFO - 01/14/25 22:36:07 - 0:03:15 - test loss 3.7358691692352295 is the best so far, saving model to checkpoint\debug\r6ivwwgibx\model.pt
INFO - 01/14/25 22:36:12 - 0:03:20 - step 3100 | loss 3.6956 | step time 43.72ms
INFO - 01/14/25 22:36:16 - 0:03:24 - step 3200 | loss 3.6443 | step time 50.80ms
INFO - 01/14/25 22:36:22 - 0:03:30 - step 3300 | loss 3.6475 | step time 53.25ms
INFO - 01/14/25 22:36:26 - 0:03:34 - step 3400 | loss 3.6674 | step time 43.31ms
INFO - 01/14/25 22:36:31 - 0:03:39 - step 3500 | loss 3.6740 | step time 43.71ms
INFO - 01/14/25 22:36:32 - 0:03:40 - step 3500 train loss: 3.643254041671753 test loss: 3.7368597984313965
INFO - 01/14/25 22:36:37 - 0:03:45 - step 3600 | loss 3.6146 | step time 48.99ms
INFO - 01/14/25 22:36:42 - 0:03:50 - step 3700 | loss 3.6530 | step time 45.10ms
INFO - 01/14/25 22:36:46 - 0:03:54 - step 3800 | loss 3.6601 | step time 47.45ms
INFO - 01/14/25 22:36:52 - 0:04:00 - step 3900 | loss 3.6132 | step time 44.34ms
INFO - 01/14/25 22:36:56 - 0:04:05 - step 4000 | loss 3.6947 | step time 41.27ms
INFO - 01/14/25 22:36:57 - 0:04:05 - step 4000 train loss: 3.63393235206604 test loss: 3.7346489429473877
INFO - 01/14/25 22:36:57 - 0:04:05 - test loss 3.7346489429473877 is the best so far, saving model to checkpoint\debug\r6ivwwgibx\model.pt
INFO - 01/14/25 22:37:02 - 0:04:10 - step 4100 | loss 3.6257 | step time 41.41ms
INFO - 01/14/25 22:37:07 - 0:04:15 - step 4200 | loss 3.6635 | step time 48.28ms
INFO - 01/14/25 22:37:11 - 0:04:19 - step 4300 | loss 3.6647 | step time 44.89ms
INFO - 01/14/25 22:37:15 - 0:04:24 - step 4400 | loss 3.6008 | step time 40.37ms
INFO - 01/14/25 22:37:20 - 0:04:28 - step 4500 | loss 3.5596 | step time 88.96ms
INFO - 01/14/25 22:37:21 - 0:04:29 - step 4500 train loss: 3.6140716075897217 test loss: 3.7402737140655518
INFO - 01/14/25 22:37:26 - 0:04:34 - step 4600 | loss 3.6047 | step time 46.05ms
INFO - 01/14/25 22:37:30 - 0:04:39 - step 4700 | loss 3.6065 | step time 48.72ms
INFO - 01/14/25 22:37:36 - 0:04:44 - step 4800 | loss 3.6367 | step time 62.81ms
INFO - 01/14/25 22:37:40 - 0:04:48 - step 4900 | loss 3.5730 | step time 46.59ms
INFO - 01/14/25 22:37:45 - 0:04:53 - step 5000 | loss 3.6365 | step time 43.36ms
INFO - 01/14/25 22:37:45 - 0:04:54 - step 5000 train loss: 3.5986592769622803 test loss: 3.7498581409454346
INFO - 01/14/25 22:37:50 - 0:04:58 - step 5100 | loss 3.5891 | step time 82.54ms
INFO - 01/14/25 22:37:55 - 0:05:03 - step 5200 | loss 3.5954 | step time 45.21ms
INFO - 01/14/25 22:37:59 - 0:05:07 - step 5300 | loss 3.5867 | step time 46.96ms
INFO - 01/14/25 22:38:03 - 0:05:12 - step 5400 | loss 3.6059 | step time 47.63ms
INFO - 01/14/25 22:38:09 - 0:05:17 - step 5500 | loss 3.6806 | step time 44.07ms
INFO - 01/14/25 22:38:09 - 0:05:18 - step 5500 train loss: 3.600850820541382 test loss: 3.76611328125
INFO - 01/14/25 22:38:14 - 0:05:22 - step 5600 | loss 3.5606 | step time 41.59ms
INFO - 01/14/25 22:38:18 - 0:05:27 - step 5700 | loss 3.6187 | step time 43.04ms
INFO - 01/14/25 22:38:23 - 0:05:32 - step 5800 | loss 3.6249 | step time 45.34ms
INFO - 01/14/25 22:38:28 - 0:05:36 - step 5900 | loss 3.5573 | step time 50.51ms
INFO - 01/14/25 22:38:32 - 0:05:41 - step 6000 | loss 3.5767 | step time 43.33ms
INFO - 01/14/25 22:38:33 - 0:05:41 - step 6000 train loss: 3.566962242126465 test loss: 3.7676491737365723
INFO - 01/14/25 22:38:38 - 0:05:47 - step 6100 | loss 3.5272 | step time 42.21ms
INFO - 01/14/25 22:38:43 - 0:05:51 - step 6200 | loss 3.5920 | step time 51.23ms
INFO - 01/14/25 22:38:47 - 0:05:55 - step 6300 | loss 3.5328 | step time 44.87ms
INFO - 01/14/25 22:38:52 - 0:06:00 - step 6400 | loss 3.5066 | step time 47.68ms
INFO - 01/14/25 22:38:57 - 0:06:05 - step 6500 | loss 3.6216 | step time 42.03ms
INFO - 01/14/25 22:38:57 - 0:06:06 - step 6500 train loss: 3.552330732345581 test loss: 3.7789969444274902
INFO - 01/14/25 22:39:02 - 0:06:10 - step 6600 | loss 3.5013 | step time 46.03ms
INFO - 01/14/25 22:39:07 - 0:06:15 - step 6700 | loss 3.5069 | step time 48.27ms
INFO - 01/14/25 22:39:11 - 0:06:20 - step 6800 | loss 3.5126 | step time 46.17ms
INFO - 01/14/25 22:39:16 - 0:06:24 - step 6900 | loss 3.5233 | step time 40.73ms
INFO - 01/14/25 22:39:21 - 0:06:29 - step 7000 | loss 3.5032 | step time 46.36ms
INFO - 01/14/25 22:39:22 - 0:06:30 - step 7000 train loss: 3.5322744846343994 test loss: 3.7903850078582764
INFO - 01/14/25 22:39:27 - 0:06:35 - step 7100 | loss 3.5144 | step time 46.03ms
INFO - 01/14/25 22:39:31 - 0:06:39 - step 7200 | loss 3.5230 | step time 42.49ms
INFO - 01/14/25 22:39:36 - 0:06:44 - step 7300 | loss 3.4655 | step time 49.14ms
INFO - 01/14/25 22:39:41 - 0:06:49 - step 7400 | loss 3.5279 | step time 42.63ms
INFO - 01/14/25 22:39:45 - 0:06:54 - step 7500 | loss 3.4487 | step time 39.11ms
INFO - 01/14/25 22:39:46 - 0:06:54 - step 7500 train loss: 3.512678623199463 test loss: 3.8085811138153076
INFO - 01/14/25 22:39:51 - 0:06:59 - step 7600 | loss 3.5158 | step time 49.12ms
INFO - 01/14/25 22:39:55 - 0:07:04 - step 7700 | loss 3.5577 | step time 46.02ms
INFO - 01/14/25 22:40:00 - 0:07:08 - step 7800 | loss 3.5105 | step time 40.01ms
INFO - 01/14/25 22:40:04 - 0:07:12 - step 7900 | loss 3.4946 | step time 42.80ms
INFO - 01/14/25 22:40:09 - 0:07:18 - step 8000 | loss 3.4708 | step time 45.12ms
INFO - 01/14/25 22:40:10 - 0:07:18 - step 8000 train loss: 3.498464345932007 test loss: 3.825451374053955
INFO - 01/14/25 22:40:15 - 0:07:23 - step 8100 | loss 3.4447 | step time 39.54ms
INFO - 01/14/25 22:40:19 - 0:07:27 - step 8200 | loss 3.6022 | step time 48.25ms
INFO - 01/14/25 22:40:25 - 0:07:34 - step 8300 | loss 3.4851 | step time 60.50ms
INFO - 01/14/25 22:40:30 - 0:07:38 - step 8400 | loss 3.4331 | step time 41.74ms
INFO - 01/14/25 22:40:34 - 0:07:42 - step 8500 | loss 3.4559 | step time 44.91ms
INFO - 01/14/25 22:40:35 - 0:07:43 - step 8500 train loss: 3.4675920009613037 test loss: 3.841001033782959
INFO - 01/14/25 22:40:39 - 0:07:47 - step 8600 | loss 3.3464 | step time 38.87ms
INFO - 01/14/25 22:40:44 - 0:07:52 - step 8700 | loss 3.5696 | step time 43.80ms
INFO - 01/14/25 22:40:48 - 0:07:57 - step 8800 | loss 3.4517 | step time 42.19ms
INFO - 01/14/25 22:40:54 - 0:08:02 - step 8900 | loss 3.4179 | step time 48.89ms
INFO - 01/14/25 22:40:59 - 0:08:07 - step 9000 | loss 3.3925 | step time 74.28ms
INFO - 01/14/25 22:40:59 - 0:08:08 - step 9000 train loss: 3.4461183547973633 test loss: 3.8714027404785156
INFO - 01/14/25 22:41:04 - 0:08:12 - step 9100 | loss 3.4447 | step time 40.59ms
INFO - 01/14/25 22:41:08 - 0:08:16 - step 9200 | loss 3.3243 | step time 44.49ms
INFO - 01/14/25 22:41:13 - 0:08:21 - step 9300 | loss 3.4704 | step time 43.64ms
INFO - 01/14/25 22:41:17 - 0:08:25 - step 9400 | loss 3.5124 | step time 40.98ms
INFO - 01/14/25 22:41:21 - 0:08:29 - step 9500 | loss 3.4187 | step time 44.37ms
INFO - 01/14/25 22:41:22 - 0:08:30 - step 9500 train loss: 3.4307165145874023 test loss: 3.886038303375244
INFO - 01/14/25 22:41:26 - 0:08:35 - step 9600 | loss 3.4107 | step time 40.04ms
INFO - 01/14/25 22:41:31 - 0:08:39 - step 9700 | loss 3.3986 | step time 41.66ms
INFO - 01/14/25 22:41:35 - 0:08:43 - step 9800 | loss 3.4177 | step time 41.55ms
INFO - 01/14/25 22:41:39 - 0:08:48 - step 9900 | loss 3.3875 | step time 43.92ms
INFO - 01/14/25 22:41:44 - 0:08:52 - step 10000 | loss 3.3011 | step time 42.30ms
INFO - 01/14/25 22:41:44 - 0:08:53 - step 10000 train loss: 3.407740831375122 test loss: 3.9165375232696533
INFO - 01/14/25 22:41:49 - 0:08:57 - step 10100 | loss 3.4360 | step time 42.63ms
INFO - 01/14/25 22:41:53 - 0:09:01 - step 10200 | loss 3.4050 | step time 44.27ms
INFO - 01/14/25 22:41:57 - 0:09:06 - step 10300 | loss 3.4688 | step time 45.00ms
INFO - 01/14/25 22:42:02 - 0:09:10 - step 10400 | loss 3.4198 | step time 42.20ms
INFO - 01/14/25 22:42:06 - 0:09:14 - step 10500 | loss 3.3715 | step time 42.78ms
INFO - 01/14/25 22:42:07 - 0:09:15 - step 10500 train loss: 3.39406418800354 test loss: 3.9336256980895996
INFO - 01/14/25 22:42:11 - 0:09:20 - step 10600 | loss 3.4011 | step time 42.97ms
INFO - 01/14/25 22:42:16 - 0:09:24 - step 10700 | loss 3.3403 | step time 42.59ms
INFO - 01/14/25 22:42:20 - 0:09:28 - step 10800 | loss 3.3710 | step time 42.71ms
INFO - 01/14/25 22:42:24 - 0:09:33 - step 10900 | loss 3.3845 | step time 40.83ms
INFO - 01/14/25 22:42:29 - 0:09:37 - step 11000 | loss 3.3766 | step time 43.24ms
INFO - 01/14/25 22:42:30 - 0:09:38 - step 11000 train loss: 3.3559682369232178 test loss: 3.964157819747925
INFO - 01/14/25 22:42:34 - 0:09:42 - step 11100 | loss 3.3626 | step time 42.69ms
INFO - 01/14/25 22:42:38 - 0:09:47 - step 11200 | loss 3.3043 | step time 43.23ms
INFO - 01/14/25 22:42:43 - 0:09:51 - step 11300 | loss 3.2607 | step time 43.77ms
INFO - 01/14/25 22:42:47 - 0:09:55 - step 11400 | loss 3.3682 | step time 45.31ms
INFO - 01/14/25 22:42:51 - 0:10:00 - step 11500 | loss 3.2841 | step time 42.84ms
INFO - 01/14/25 22:42:52 - 0:10:00 - step 11500 train loss: 3.356403350830078 test loss: 3.991460084915161
INFO - 01/14/25 22:42:56 - 0:10:05 - step 11600 | loss 3.3579 | step time 45.90ms
INFO - 01/14/25 22:43:01 - 0:10:09 - step 11700 | loss 3.3519 | step time 42.72ms
INFO - 01/14/25 22:43:05 - 0:10:13 - step 11800 | loss 3.3303 | step time 42.56ms
INFO - 01/14/25 22:43:10 - 0:10:18 - step 11900 | loss 3.2884 | step time 42.96ms
INFO - 01/14/25 22:43:14 - 0:10:22 - step 12000 | loss 3.2541 | step time 44.09ms
INFO - 01/14/25 22:43:15 - 0:10:23 - step 12000 train loss: 3.3249475955963135 test loss: 4.020266532897949
INFO - 01/14/25 22:43:19 - 0:10:27 - step 12100 | loss 3.2478 | step time 41.61ms
INFO - 01/14/25 22:43:24 - 0:10:32 - step 12200 | loss 3.4049 | step time 43.56ms
INFO - 01/14/25 22:43:28 - 0:10:36 - step 12300 | loss 3.2516 | step time 50.52ms
INFO - 01/14/25 22:43:32 - 0:10:40 - step 12400 | loss 3.2335 | step time 40.43ms
INFO - 01/14/25 22:43:37 - 0:10:45 - step 12500 | loss 3.2631 | step time 45.20ms
INFO - 01/14/25 22:43:37 - 0:10:46 - step 12500 train loss: 3.301011085510254 test loss: 4.028707504272461
INFO - 01/14/25 22:43:42 - 0:10:50 - step 12600 | loss 3.2547 | step time 42.76ms
INFO - 01/14/25 22:43:46 - 0:10:54 - step 12700 | loss 3.3132 | step time 43.23ms
INFO - 01/14/25 22:43:50 - 0:10:59 - step 12800 | loss 3.3051 | step time 45.92ms
INFO - 01/14/25 22:43:55 - 0:11:03 - step 12900 | loss 3.1972 | step time 43.71ms
INFO - 01/14/25 22:43:59 - 0:11:07 - step 13000 | loss 3.3326 | step time 45.07ms
INFO - 01/14/25 22:44:00 - 0:11:08 - step 13000 train loss: 3.279740810394287 test loss: 4.063748359680176
INFO - 01/14/25 22:44:04 - 0:11:13 - step 13100 | loss 3.3625 | step time 45.37ms
INFO - 01/14/25 22:44:09 - 0:11:17 - step 13200 | loss 3.0935 | step time 45.29ms
INFO - 01/14/25 22:44:13 - 0:11:22 - step 13300 | loss 3.2993 | step time 46.72ms
INFO - 01/14/25 22:44:18 - 0:11:26 - step 13400 | loss 3.2918 | step time 42.93ms
INFO - 01/14/25 22:44:22 - 0:11:30 - step 13500 | loss 3.2165 | step time 46.40ms
INFO - 01/14/25 22:44:23 - 0:11:31 - step 13500 train loss: 3.2550384998321533 test loss: 4.081221580505371
INFO - 01/14/25 22:44:27 - 0:11:35 - step 13600 | loss 3.2051 | step time 44.49ms
INFO - 01/14/25 22:44:32 - 0:11:40 - step 13700 | loss 3.1859 | step time 41.20ms
INFO - 01/14/25 22:44:36 - 0:11:44 - step 13800 | loss 3.3140 | step time 40.21ms
INFO - 01/14/25 22:44:40 - 0:11:49 - step 13900 | loss 3.2323 | step time 41.45ms
INFO - 01/14/25 22:44:45 - 0:11:53 - step 14000 | loss 3.2096 | step time 44.38ms
INFO - 01/14/25 22:44:46 - 0:11:54 - step 14000 train loss: 3.237823724746704 test loss: 4.116389751434326
INFO - 01/14/25 22:44:50 - 0:11:58 - step 14100 | loss 3.2630 | step time 47.23ms
INFO - 01/14/25 22:44:54 - 0:12:02 - step 14200 | loss 3.2776 | step time 45.36ms
INFO - 01/14/25 22:44:59 - 0:12:07 - step 14300 | loss 3.2109 | step time 41.31ms
INFO - 01/14/25 22:45:03 - 0:12:11 - step 14400 | loss 3.1884 | step time 42.32ms
INFO - 01/14/25 22:45:07 - 0:12:16 - step 14500 | loss 3.2560 | step time 41.52ms
INFO - 01/14/25 22:45:08 - 0:12:16 - step 14500 train loss: 3.2262611389160156 test loss: 4.143389701843262
INFO - 01/14/25 22:45:13 - 0:12:21 - step 14600 | loss 3.3351 | step time 40.88ms
INFO - 01/14/25 22:45:17 - 0:12:25 - step 14700 | loss 3.1547 | step time 41.91ms
INFO - 01/14/25 22:45:21 - 0:12:30 - step 14800 | loss 3.2187 | step time 44.46ms
INFO - 01/14/25 22:45:26 - 0:12:34 - step 14900 | loss 3.1937 | step time 46.24ms
INFO - 01/14/25 22:45:30 - 0:12:38 - step 15000 | loss 3.1878 | step time 43.11ms
INFO - 01/14/25 22:45:31 - 0:12:39 - step 15000 train loss: 3.195587396621704 test loss: 4.15415620803833
INFO - 01/14/25 22:45:35 - 0:12:43 - step 15100 | loss 3.2280 | step time 42.31ms
INFO - 01/14/25 22:45:40 - 0:12:48 - step 15200 | loss 3.1097 | step time 44.63ms
INFO - 01/14/25 22:45:44 - 0:12:52 - step 15300 | loss 3.1752 | step time 46.48ms
INFO - 01/14/25 22:45:48 - 0:12:57 - step 15400 | loss 3.2698 | step time 42.48ms
INFO - 01/14/25 22:45:53 - 0:13:01 - step 15500 | loss 3.2548 | step time 44.46ms
INFO - 01/14/25 22:45:54 - 0:13:02 - step 15500 train loss: 3.1835663318634033 test loss: 4.174502372741699
INFO - 01/14/25 22:45:58 - 0:13:06 - step 15600 | loss 3.1671 | step time 46.42ms
INFO - 01/14/25 22:46:02 - 0:13:10 - step 15700 | loss 3.2032 | step time 41.43ms
INFO - 01/14/25 22:46:07 - 0:13:15 - step 15800 | loss 3.2369 | step time 41.79ms
INFO - 01/14/25 22:46:11 - 0:13:19 - step 15900 | loss 3.2577 | step time 45.39ms
INFO - 01/14/25 22:46:15 - 0:13:24 - step 16000 | loss 3.2289 | step time 42.05ms
INFO - 01/14/25 22:46:16 - 0:13:24 - step 16000 train loss: 3.1685285568237305 test loss: 4.195828437805176
INFO - 01/14/25 22:46:20 - 0:13:29 - step 16100 | loss 3.2273 | step time 42.14ms
INFO - 01/14/25 22:46:25 - 0:13:33 - step 16200 | loss 3.2277 | step time 44.12ms
INFO - 01/14/25 22:46:29 - 0:13:37 - step 16300 | loss 3.3289 | step time 42.33ms
INFO - 01/14/25 22:46:36 - 0:13:44 - step 16400 | loss 3.1627 | step time 43.57ms
INFO - 01/14/25 22:46:40 - 0:13:48 - step 16500 | loss 3.1625 | step time 43.44ms
INFO - 01/14/25 22:46:41 - 0:13:49 - step 16500 train loss: 3.168124198913574 test loss: 4.198256969451904
INFO - 01/14/25 22:46:45 - 0:13:54 - step 16600 | loss 3.1165 | step time 42.42ms
INFO - 01/14/25 22:46:50 - 0:13:58 - step 16700 | loss 3.1736 | step time 43.99ms
INFO - 01/14/25 22:46:54 - 0:14:02 - step 16800 | loss 3.0752 | step time 41.70ms
INFO - 01/14/25 22:46:58 - 0:14:07 - step 16900 | loss 3.2352 | step time 40.48ms
INFO - 01/14/25 22:47:03 - 0:14:11 - step 17000 | loss 3.1409 | step time 43.61ms
INFO - 01/14/25 22:47:04 - 0:14:12 - step 17000 train loss: 3.1321332454681396 test loss: 4.233092784881592
INFO - 01/14/25 22:47:08 - 0:14:16 - step 17100 | loss 3.0820 | step time 45.48ms
INFO - 01/14/25 22:47:12 - 0:14:20 - step 17200 | loss 3.1040 | step time 44.60ms
INFO - 01/14/25 22:47:17 - 0:14:25 - step 17300 | loss 3.1166 | step time 46.82ms
INFO - 01/14/25 22:47:21 - 0:14:29 - step 17400 | loss 3.1137 | step time 42.07ms
INFO - 01/14/25 22:47:25 - 0:14:34 - step 17500 | loss 3.0873 | step time 43.41ms
INFO - 01/14/25 22:47:26 - 0:14:34 - step 17500 train loss: 3.1308372020721436 test loss: 4.255462646484375
INFO - 01/14/25 22:47:30 - 0:14:39 - step 17600 | loss 3.1541 | step time 46.39ms
INFO - 01/14/25 22:47:35 - 0:14:43 - step 17700 | loss 3.1328 | step time 44.03ms
INFO - 01/14/25 22:47:39 - 0:14:47 - step 17800 | loss 3.1549 | step time 41.71ms
INFO - 01/14/25 22:47:43 - 0:14:52 - step 17900 | loss 3.0989 | step time 42.35ms
INFO - 01/14/25 22:47:48 - 0:14:56 - step 18000 | loss 3.1310 | step time 49.63ms
INFO - 01/14/25 22:47:49 - 0:14:57 - step 18000 train loss: 3.1246793270111084 test loss: 4.277778625488281
INFO - 01/14/25 22:47:53 - 0:15:01 - step 18100 | loss 3.1686 | step time 48.69ms
INFO - 01/14/25 22:47:57 - 0:15:06 - step 18200 | loss 3.1383 | step time 39.92ms
INFO - 01/14/25 22:48:02 - 0:15:10 - step 18300 | loss 3.1070 | step time 45.57ms
INFO - 01/14/25 22:48:06 - 0:15:14 - step 18400 | loss 3.0639 | step time 41.47ms
INFO - 01/14/25 22:48:10 - 0:15:19 - step 18500 | loss 3.0357 | step time 45.22ms
INFO - 01/14/25 22:48:11 - 0:15:19 - step 18500 train loss: 3.1073806285858154 test loss: 4.304169654846191
INFO - 01/14/25 22:48:16 - 0:15:24 - step 18600 | loss 3.0930 | step time 43.35ms
INFO - 01/14/25 22:48:20 - 0:15:28 - step 18700 | loss 3.0792 | step time 42.75ms
INFO - 01/14/25 22:48:24 - 0:15:33 - step 18800 | loss 3.1364 | step time 42.35ms
INFO - 01/14/25 22:48:29 - 0:15:37 - step 18900 | loss 3.0304 | step time 43.79ms
INFO - 01/14/25 22:48:33 - 0:15:41 - step 19000 | loss 3.0416 | step time 48.97ms
INFO - 01/14/25 22:48:34 - 0:15:42 - step 19000 train loss: 3.0802268981933594 test loss: 4.332536697387695
INFO - 01/14/25 22:48:38 - 0:15:46 - step 19100 | loss 3.0533 | step time 43.02ms
INFO - 01/14/25 22:48:43 - 0:15:51 - step 19200 | loss 3.0580 | step time 41.52ms
INFO - 01/14/25 22:48:47 - 0:15:55 - step 19300 | loss 3.0667 | step time 46.18ms
INFO - 01/14/25 22:48:51 - 0:16:00 - step 19400 | loss 3.0221 | step time 44.56ms
INFO - 01/14/25 22:48:56 - 0:16:04 - step 19500 | loss 3.0330 | step time 48.31ms
INFO - 01/14/25 22:48:56 - 0:16:05 - step 19500 train loss: 3.07528018951416 test loss: 4.328799247741699
INFO - 01/14/25 22:49:01 - 0:16:09 - step 19600 | loss 3.0696 | step time 44.44ms
INFO - 01/14/25 22:49:05 - 0:16:13 - step 19700 | loss 2.9463 | step time 45.88ms
INFO - 01/14/25 22:49:10 - 0:16:18 - step 19800 | loss 3.0383 | step time 43.53ms
INFO - 01/14/25 22:49:14 - 0:16:22 - step 19900 | loss 3.0487 | step time 44.41ms
INFO - 01/14/25 22:49:18 - 0:16:26 - Memory allocated:  22.15MB, reserved: 72.00MB
INFO - 01/14/25 22:49:18 - 0:16:26 - generating
INFO - 01/14/25 22:49:18 - 0:16:26 - 40000 samples remaining
